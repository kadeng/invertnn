{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%python\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Hello\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "# Invertible Networks\n",
    "\n",
    "The invertnn library contains many implementations of invertible or bijective layer types & activations for the construction of Invertible Neural Networks (INNs). For a reference of the available classes, see\n",
    "\n",
    " * [Invertible Transformations](../api_ref.html#module-invertnn.pytorch.invertible_transforms)\n",
    " * [Orthogonal Transformation](../api_ref.html#module-invertnn.pytorch.orthogonal_transform)\n",
    "\n",
    "## Why invert Neural Networks ?\n",
    "\n",
    "There are several uses for invertible Neural Networks. The most important are in the field of Probabilistic Modeling, Variational Inference and Generative Adversarial Networks (GANs).\n",
    "\n",
    "In the context of GANs, but also in Variational Autoencoders (VAEs), a part of the model consists of a Generator Network $G$, which is a deterministic function which maps (in the most common form, without loss of generality) a random vector $Z \\in \\mathbb{R}^n$ to a transformed random vector $X \\in \\mathbb{R}^m$.\n",
    "\n",
    "The random vector $Z$ usually follows a known parametric distribution, e.g $Z \\sim P_z$, such as a multivariate normal with diagonal covariance matrix or similar. The parameters of this distribution ( $\\theta_z$ ) can again be the function of some input (for example in VAEs or conditional models). But what is important here is that  the distribution of $Z$ and specifically the logarithm of it's probability density function $\\log{p(z)}$ can be calculated efficiently. \n",
    "\n",
    "If the function $G$ would be a smooth and invertible function, we could apply the so called **Change of Variables** technique (see paragraph below, or [Wikipedia Article](https://en.wikipedia.org/wiki/Probability_density_function#Dependent_variables_and_change_of_variables)) to calculate the density of $G(Z)$ given the density of $Z$ and the determinant of the jacobian of the inverse mapping.\n",
    "\n",
    "Therefore, if we can construct and train invertible neural networks while retaining their universal function approximator property, we can therefore approximate arbitrary probability densities. Not only that, but we can also sample from them and reconstruct the latent code (e.g. $Z$) which has been used to generate any sample $X$.\n",
    "\n",
    "If we can do that, we can do both: Apply techniques from Variational Inference, such as divergence minimization to minimize the divergence between the empirical distribution of the reconstructed latent code $\\hat{Z} = G^{-1}(X)$, e.g. we assume $\\hat{Z} \\sim Q_{Z}$ and can attempt to minimize, for example, the KL-Divergence ${KL}(Q_z || P_z )$.\n",
    "\n",
    "We can also apply methods from Generative Adversarial Learning, using Discriminator Functions or Critics to train the Generator Network $G$ to generate samples $\\hat{X} = G(z)$ where we have both:\n",
    "\n",
    " * The distribution of $\\hat{X}$ is indistinguishable from the distribution of observed training examples $X$\n",
    " * The inverse empirical distribution of $\\hat{Z}$ is indistinguishable from the distribution $P_z$\n",
    " \n",
    "\n",
    "### Change of Variables Technique\n",
    "\n",
    "The following paragraph is a short extract from the [Wikipedia Article on Random Variables](https://en.wikipedia.org/wiki/Probability_density_function)\n",
    "\n",
    "If the probability density function of a random variable $X$ is given as $f_X(x)$, it is possible (but often not necessary; see below) to calculate the probability density function of some variable $Y = g(X)$. This is also called a “change of variable” and is in practice used to generate a random variable of arbitrary shape $f_{g(X)} = f_Y$  using a known (for instance, uniform) random number generator.\n",
    "\n",
    "If the function $g$ is monotonic, then the resulting density function is\n",
    "\n",
    "$$\n",
    "f_Y(y) = \\left| \\frac{d}{dy} \\big(g^{-1}(y)\\big) \\right| \\cdot f_X\\big(g^{-1}(y)\\big)\n",
    "$$\n",
    "\n",
    "Here $g^{−1}$ denotes the inverse function.\n",
    "\n",
    "This follows from the fact that the probability contained in a differential area must be invariant under change of variables. That is,\n",
    "\n",
    "$$\n",
    "\\left| f_Y(y)\\, dy \\right| = \\left| f_X(x)\\, dx \\right|\n",
    "$$\n",
    "\n",
    "This technique can obviously also be applied in a chained-manner, for example in a layered neural network, each\n",
    "layer could calculate the determinant of the jacobian of the inverse function and just multiply that.\n",
    "\n",
    "This leads us to the following question:\n",
    "\n",
    "## Which kinds of Neural Networks are invertible ?\n",
    "\n",
    "For practical purposes, we require the inversion to be tractable and efficient, and require that it is possible to easily calculate gradients with respect to the network parameters in both directions, as well as calculate the log of the absolute value of the determinant of the jacobian in the inverse direction. **It should be approximately equally efficient to forward and backward propagate through the network in either direction** \n",
    "\n",
    "Obviously, compositions of invertible functions are invertible themselves. Therefore, we can tackle this problem layerwise. \n",
    "\n",
    "Within invertnn, we require invertible layers to conform to a well defined **interface**. They need to be subclasses of **torch.nn.Module** and also derive from the following (partially) abstract base class:\n",
    "\n",
    "```python\n",
    "class InvertibleModule(object,  metaclass=abc.ABCMeta):\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def invert(self, output):\n",
    "        \"\"\"Calculate the inverted transformation of forward\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def inv_jacobian_logabsdet(self, output):\n",
    "        \"\"\"\n",
    "        Log of the absolute value of the derminant of the jacobian of\n",
    "        the inverted transform at output\n",
    "\n",
    "        returns: inverted output, log of absolute value of jacobian\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def inverted_module(self):\n",
    "        '''Return a module that's the inversion of this module'''\n",
    "        return InvertedModule(self)\n",
    "\n",
    "```\n",
    "\n",
    "In order to make it a bit simpler, we have certain mixin classes, which make it easier to add default functionality for layers which either do not change the density (i.e. they are \"volume preserving\"):\n",
    "\n",
    "```python\n",
    "class InvertibleVolumePreservingMixin(object):\n",
    "\n",
    "    def inv_jacobian_logabsdet(self, output):\n",
    "        \"\"\"\n",
    "        Log of the absolute valie of the derminant of the jacobian of\n",
    "        the inverted transform at output\n",
    "        \"\"\"\n",
    "        return self.invert(output), torch.ones((output.shape[0], 1)).to(output.device)\n",
    "```\n",
    "\n",
    "or for those Layer types which apply a purely componentwise transformation, which makes it possible to work with the gradient, instead of the full Jacobian:\n",
    "\n",
    "```python\n",
    "\n",
    "class InvertibleComponentwiseMixin(object):\n",
    "\n",
    "    def inv_jacobian_logabsdet(self, output):\n",
    "        \"\"\"\n",
    "        Log of the absolute valie of the derminant of the jacobian of\n",
    "        the inverted transform at output\n",
    "        \"\"\"\n",
    "        ovar = autograd.Variable(output.detach(), requires_grad=True)\n",
    "        inverse = self.invert(ovar)\n",
    "        grad = autograd.grad([inverse], [ovar], [ torch.ones_like(inverse) ])\n",
    "        jacobian_log_abs_det = torch.sum(grad[0].abs().log().view(output.shape[0], -1), dim=1)\n",
    "        return inverse, jacobian_log_abs_det\n",
    "```\n",
    "\n",
    "Based on this functionality, some invertible transformations can be implemented rather trivially, such as in this example:\n",
    "\n",
    "```python\n",
    "class InvertibleTanh(InvertibleComponentwiseMixin, InvertibleModule, nn.Tanh):\n",
    "\n",
    "    def invert(self, output):\n",
    "        return 0.5 * (torch.log(1+output) - torch.log(1-output))\n",
    "```\n",
    "\n",
    "### Invertible Activation Functions\n",
    "\n",
    "For an activation function to be invertible, it needs to be strictly monotonic. This is true for, for example, the *Sigmoid*, *Tanh* and *LeakyReLU* activation functions, but not for *ReLUs*. For the aforementioned, we have corresponding activation functions in the [Invertible Transformations Module](../api_ref.html#module-invertnn.pytorch.invertible_transforms) of invertnn.\n",
    "\n",
    "The following invertible Activation Functions have been implemented in invertnn:\n",
    "\n",
    " * [Invertible Sigmoid](../api_ref.html#invertnn.pytorch.invertible_transforms.InvertibleSigmoid)\n",
    " * [Invertible Tanh](../api_ref.html#invertnn.pytorch.invertible_transforms.InvertibleTanh)\n",
    " * [Invertible LeakyReLU](../api_ref.html#invertnn.pytorch.invertible_transforms.InvertibleLeakyReLU)\n",
    " * [Invertible Baird Activation](../api_ref.html#invertnn.pytorch.invertible_transforms.InvertibleBairdActivation) - see [Baird et. al: One-Step Neural Network Inversion with PDF\n",
    "Learning and Emulation](http://leemon.com/papers/2005bsi.pdf)\n",
    " \n",
    "Please note that we recommend against using LeakyReLU if you want to learn probability distortions, given that their curvature is either zero or undefined everywhere, therefore we have no informative gradient with respect to how to increase or decrease the density.\n",
    "\n",
    "### Invertible Common Layer Types\n",
    "\n",
    " * [Invertible Sequential](../api_ref.html#invertnn.pytorch.invertible_transforms.InvertibleSequential) - the inverse here is to just apply the inverse in reversed order and sum the jacobian log abs determinant.\n",
    " * [Invertible Shuffle](../api_ref.html#invertnn.pytorch.invertible_transforms.InvertibleShuffle) - An invertible shuffling operation. Useful in combination with Coupling Layers (see further below)\n",
    " * [Invertible PixelShuffle](../api_ref.html#invertnn.pytorch.invertible_transforms.InvertiblePixelShuffle) - Invertible Variant of [PixelShuffle Operation](https://pytorch.org/docs/master/nn.html#torch.nn.PixelShuffle) - which can be used to implement Deconvolution-Like Operations. Basically it trades channel depth for resolution in one direction or another.\n",
    " * [Invertible Concat](../api_ref.html#invertnn.pytorch.invertible_transforms.InvertibleConcat) - On the forward pass, this concats additional input. On the inverse pass, it removes and stores that additional input into the \"restored_input\" property.\n",
    " * [Invertible Concat Noise](../api_ref.html#invertnn.pytorch.invertible_transforms.InvertibleConcatNoise) - Like invertible concat, but on every forward pass, it samples the input to concat from an instance of **torch.distributions.Distribution** and restores it on inverse pass ( subtracting the log propability of the restored input from the log abs det of the jacobian )\n",
    " \n",
    "### Adding & Extracting (White-) Noise \n",
    "\n",
    "An invertible network has to introduce or extract random noise, whenever the degrees of freedom increase or decrease (e.g. if the number of neurons, or, for convolutional networks, CHANNELS x WIDTH x HEIGHT ).\n",
    "\n",
    "When injecting noise, we usually first generate randomness by repeatedly drawing independent samples from **univariate distributions** such as uniform, univariate gaussian or bernoulli distributions. \n",
    "\n",
    "On the backward pass, the independence assumption is very likely not to hold anymore. We cannot expect the noise we extract on the backward pass to be componentwise independent from each other. To be realistic, the noise we get back is probably highly correlated. \n",
    "\n",
    "If we then **assume independence** and calculate things like KL-Divergence or do maximum likelihood learning it will simply not work well, because the assumption is severely violated.\n",
    "\n",
    "One approach to solve this, is to apply a so called **Whitening Transform**. Whitening means, we decorrelate the samples. This does not give us true statistical independence, but we're getting much closer.\n",
    "\n",
    "More on Statistical Whitening can be found in this [excellent blog post on statistical whitening by Joe Louis Marino (archived on archive.org)](https://web.archive.org/web/20180813034201/http://joelouismarino.github.io/blog_posts/blog_whitening.html)\n",
    "\n",
    "As can be seen from the above post, whitening can be implemented by multiplying random noise vectors with, in the case of PCA Whitening, an \n",
    "orthogonal and a diagonal matrix, or in the case of ZCA Whitening, by multiplying with an orthogonal, a diagonal and the \n",
    "transpose of the first orthogonal Matrix.\n",
    "\n",
    "Luckily, all of these are invertible operations. We just need to find the right matrices.\n",
    "\n",
    "In order to make it easier to inject & extract the right kind of noise from the Network,  invertnn provides the class\n",
    "[AdaptiveInverseWhiteningTransformer](../api_ref.html#invertnn.pytorch.white_noise.AdaptiveInverseWhiteningTransformer)\n",
    "\n",
    "This class can provide a parametric **torch.distributions.Transformer**, which can correlate statistical noise on the\n",
    "forward pass (when injecting noise) and decorrelate it on the way back (when reconstructing the original, independent, noise).\n",
    "\n",
    "\n",
    "### Coupling Layers\n",
    "\n",
    "The [Invertible Coupling Layer](../api_ref.html#invertnn.pytorch.invertible_transforms.InvertibleCoupling Layer) is an implementation of the so called Coupling Layer from [Dinh. et al: Density estimation using Real NVP](https://arxiv.org/pdf/1605.08803.pdf).\n",
    "\n",
    "This coupling layer can be applied to both affine (i.e. 1D, 2D, 3D input) or plain input. It operates on a per-channel basis. If we have **D** input channels, it will couple the first **d** layers with the last **D-d** layers using arbitrary nonlinear transformation networks $s: \\mathbb{R}^d \\mapsto \\mathbb{R}^{D-d}$ and $s: \\mathbb{R}^d \\mapsto \\mathbb{R}^{D-d}$. The output will consist of *d* unchanged channels, and ** D-d** coupled channels.\n",
    "        \n",
    "More on these coupling layers is to follow. For now, please refer to the paper above. \n",
    "\n",
    "### Invertible Linear Layers (Orthogonal, Diagonal & Singular Value Composition Layers)\n",
    "\n",
    "The following Layers are intended to be more or less Drop-In Replacements for (square) Linear Layers, as well as for Pixelwise-Convolutions with a Kernel-Size of 1.\n",
    "\n",
    " * [Orthogonal Transform](../api_ref.html#invertnn.pytorch.orthogonal_transform.OrthogonalTransform) - Orthogonal Transformation using parametrization as product of Householder Reflectors.\n",
    " * [Orthogonal Transform 2D](../api_ref.html#invertnn.pytorch.orthogonal_transform.OrthogonalTransform2D) - Orthogonal Spatial Transformation using parametrization as product of Householder Reflectors.\n",
    " * [Diagonal Linear Transform](../api_ref.html#invertnn.pytorch.orthogonal_transform.DiagonalLinearTransform) - Diagonal Matrix Transformation.\n",
    " * [Diagonal Linear Transform 2D](../api_ref.html#invertnn.pytorch.orthogonal_transform.DiagonalLinearTransform2D) - Diagonal Spatial Transformation.\n",
    " \n",
    "Also, these layers can be composed ( for example into a SVD based composition ) using the [InvertibleSequential]((../api_ref.html#invertnn.pytorch.invertible_transforms.InvertibleSequential) Layer.\n",
    "\n",
    "These Layer types are one of the core contributions of the invertnn library so far. It is mostly based on the Orthogonal and SVD Reparametrizations from these papers: [Mhammedi et. al: Efficient Orthogonal Parametrisation of Recurrent Neural Networks Using Householder Reflections](https://arxiv.org/pdf/1612.00188.pdf) as well as [Zhang et al.: Stabilizing Gradients for Deep Neural Networks via Efficient SVD Parameterization](https://arxiv.org/pdf/1803.09327.pdf)\n",
    "\n",
    "##### Background for Orthogonal Layers\n",
    "\n",
    "These layers enable invertible Neural Networks in the sense we described it above, and without most of the limitations imposed by the Coupling Layers of the RealNVP Paper.\n",
    "\n",
    "Now, some Background\n",
    "\n",
    "#### Orthogonal Matrices\n",
    "\n",
    "An [Orthogonal Matrix](https://en.wikipedia.org/wiki/Orthogonal_matrix) is a matrix whose inverse is also the transpose. Which means it is **trivially invertible**. Multiplication with an Orthogonal Matrix preserves distances and angles, and is therefore **Volume Preserving**, which means it is also trivial to calculate the jacobian log determinant (always 1). \n",
    "\n",
    "#### Diagonal Matrices\n",
    "\n",
    "A **Diagonal Matrix** in turn is a Matrix with zeros everywhere, except on the diagonal. It has a compact representation if we store only the diagonal elements. It's inverse is just the componentwise reciprocal, therefore it is also **trivially invertible** (even though we can have division by zero or numerical problems if we get coefficients close to zero). Multiplication with a compactly represented Diagonal Matrix is also efficient, we can use the compontwise tensor product for that. Likewise, **calculation of the log of the determinant of the Jacobian is trivial**.\n",
    "\n",
    "#### SIngular Value Decomposition\n",
    "\n",
    "Using Singular Value Decomposition (SVD) **Any real Matrix** can be decomposed via stable numerical methods into the product of orthogonal and diagonal matrices. And if the original matrix was square, this also allows us to invert the matrix by taking the transpose of the orthogonal matrices, the inverse of the diagonal matrix and applying it in inverse order. If the original matrix was not square, we will, at least, get a least-squares solution to the inverse, if a solution is possible.\n",
    "\n",
    "#### QR Decomposition using Householder Reflectors\n",
    "\n",
    "The **QR** decomposition algorithm based on Householder matrices allows to factorize any square matrix (with shape $ N x N $) into the product of an Orthogonal Matrix **Q** and an upper triangular matrix **R**. As a by-product, we also get a factorization of the Orthogonal Matrix Q into $N$ so called householder reflectors, each of which can be represented as a so called Householder matrix of shape (N x N), or more compactly as a vector of shape N.\n",
    "\n",
    "If the original Matrix was orthogonal to begin with, we can use this to factorize it completely into Householder Matrices (i.e the resulting Matrix $R$ is provably going to be the identity Matrix).\n",
    "\n",
    "Using just some trivial constant constraints on the householder reflection vectors (some entries need to be zero, or are limited to -1 or +1), the remaining free parameters can be varied arbitrarily, and the product can be **any** Orthogonal Matrix.\n",
    "\n",
    "This means, these Householder Reflectors are suitable as a reparametrization method for Orthogonal Matrices.\n",
    "\n",
    "#### SIngular Value Composition\n",
    "\n",
    "That is, **we can factorize any real Matrix** of shape (N x M) into the product of **N** Householder Reflection Vectors of Shape **N**, one Diagonal Matrix represented as a Vector of Shape N, and **M** Householder Reflection Vectors of Shape **M**.\n",
    "\n",
    "This again means, we can construct a reparametrization of any square matrix, which is trivially invertible and fulfills our requirements for an Invertible Transformation. **It is approximately equally efficient to forward and backward propagate through the network in either direction**. Also, the parametrization is universal in the sense that it is smooth, and can represent any real matrix.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Related Papers\n",
    "\n",
    "### Density Estimation using Invertible Neural Networks\n",
    "\n",
    "It is possible to learn and approximate probability density functions using invertible neural networks based on the fact that we can apply the change of variable method \n",
    "\n",
    ". In [Baird et. al: One-Step Neural Network Inversion with PDF\n",
    "Learning and Emulation](http://leemon.com/papers/2005bsi.pdf)\n",
    "\n",
    "Baird et. al demonstrate the applicability of the Change of Variables Technique to learn PDFs, given invertible neural networks. They introduce a novel invertible activation function (available in invertnn as **InvertibleBairdActivation** in the **invertible_transforms** module) They also develop a PDF learning algorithm which we do not use.\n",
    "\n",
    " * [Dinh. et al: Density estimation using Real NVP](https://arxiv.org/pdf/1605.08803.pdf)\n",
    "Dinh et al. introduce a new method for the construction of Invertible Neural Networks using so called **Coupling Layers** (implemented in invertnn as class **InvertibleCouplingLayer** in the **invertible_transforms** module). They also demonstrate the applicability of these inverted networks on Image Generation Tasks.\n",
    "\n",
    "### Parametrization of Orthogonal Matrices using Householder Reflectors\n",
    "\n",
    "[Mhammedi et. al: Efficient Orthogonal Parametrisation of Recurrent Neural Networks Using Householder Reflections](https://arxiv.org/pdf/1612.00188.pdf)\n",
    "\n",
    "[Zhang et al.: Stabilizing Gradients for Deep Neural Networks via Efficient SVD Parameterization](https://arxiv.org/pdf/1803.09327.pdf)\n",
    "\n",
    "### GAN Architectures involving approximate inversions / decoder architectures\n",
    "\n",
    " * ALi [Dumoulin et. al: Adversarially Learned Inference](https://arxiv.org/abs/1606.00704)\n",
    " * BiGAN: [Donahue et. al: Adversarial Feature Learning](https://arxiv.org/abs/1605.09782)\n",
    " * InfoGAN: [Chen et. al: InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets](https://arxiv.org/abs/1606.03657)\n",
    "\n",
    "### Variational Autoencoders\n",
    "\n",
    " * VAEs: [Kingma & Welling: Auto-Encoding Variational Bayes\n",
    "](https://arxiv.org/abs/1312.6114)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.distributions as dists\n",
    "import invertnn.pytorch.invertible_transforms as invertible\n",
    "import invertnn.pytorch.orthogonal_transform as ortho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.9287e-22], dtype=torch.float64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sigmoid(torch.ones(1, dtype=torch.float64)*-50.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-17221d20c45a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m                 \u001b[0minvertible\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvertibleShuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m14\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m18\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m11\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m13\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m17\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m19\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m                 invertible.InvertibleCouplingLayer(8, 12,\n\u001b[1;32m----> 8\u001b[1;33m                                                    \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_mlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal_act\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSoftsign\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m                                                    self.create_mlp(8, 12, final_act=nn.Softsign)),\n\u001b[0;32m     10\u001b[0m                 \u001b[0minvertible\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvertibleShuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m14\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m18\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m11\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m13\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m17\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m19\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "model = invertible.InvertibleSequential(\n",
    "            *[ortho.OrthogonalTransform(10, bias=True), ortho.DiagonalLinearTransform(10, bias=False),\n",
    "                ortho.OrthogonalTransform(10, bias=True), invertible.InvertibleBairdActivation(layer_shape=(10,)),\n",
    "                ortho.OrthogonalTransform(10, bias=True), invertible.InvertibleShuffle(reversed(range(10))),\n",
    "                invertible.InvertibleConcat(10, 10), ortho.DiagonalLinearTransform(20, bias=False),\n",
    "                invertible.InvertibleShuffle([0,2,4,6,8,10,12,14,16,18,1,3,5,7,9,11,13,15,17,19]),\n",
    "                invertible.InvertibleCouplingLayer(8, 12,\n",
    "                                                   self.create_mlp(8, 12, final_act=nn.Softsign),\n",
    "                                                   self.create_mlp(8, 12, final_act=nn.Softsign)),\n",
    "                invertible.InvertibleShuffle([0,2,4,6,8,10,12,14,16,18,1,3,5,7,9,11,13,15,17,19]),\n",
    "                invertible.InvertibleCouplingLayer(12, 8,\n",
    "                                                   self.create_mlp(12, 8, final_act=nn.Softsign),\n",
    "                                                   self.create_mlp(12, 8, final_act=nn.Softsign)),\n",
    "                ortho.OrthogonalTransform(20, bias=True), invertible.InvertibleLeakyReLU(), ])\n",
    "model._modules['6'].input_b = torch.eye(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-017a3b7d4b69>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "a = torch.randn((10, 2))\n",
    "b = model.forward(a)\n",
    "c = model.invert(b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "Author: \n",
    "    *[Kai Londenberg](Kai.Londenberg@googlemail.com), 2018*"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "stem_cell": {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
